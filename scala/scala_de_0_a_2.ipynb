{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready\n"
     ]
    }
   ],
   "source": [
    "println(\"ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -3) Créer une sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -3.1) Importer les méthodes de conversion de type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -2) imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -1) refaire un spark context personnalisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.SparkConf@3a915864"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val appName = \"rennes\"\n",
    "val master  = \"local[3]\"\n",
    "val conf    = new SparkConf()\n",
    "conf.setAppName(appName)\n",
    "conf.setMaster(master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "local[*]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark3 = SparkSession.builder().master(\"local[3]\").appName(\"Word Count\").getOrCreate()\n",
    "val sc3    = spark3.sparkContext\n",
    "sc3.master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Créer une première rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Création de RDD par lecture d'un fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3884"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path      = \"./logs/trajets.csv\"\n",
    "val trajets   = sc.textFile(path)    // trajets est une (map)RDD\n",
    "trajets.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2) on récupère plusieurs tableaux \"sans discontinuités\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(aefb2,cycliste_azex3,velo_azed9_1486042024.4,1486042052.49,1486042054.41,1.92083001137,1.41421356237,azyio)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajets.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(aefb2,cycliste_azex3,velo_azed9_1486042024.4,1486042052.49,1486042054.41,1.92083001137,1.41421356237,azyio, azux9,cycliste_azet7,velo_aepj0_1486042024.44,1486042052.5,1486042054.41,1.91730213165,3.16227766017,aefb2, aelmx,cycliste_azem8,velo_aetin_1486042024.34,1486042053.73,1486042054.41,0.683007001877,1.0,azys7, azrop,cycliste_azeu0,velo_ardb4_1486042024.19,1486042053.74,1486042054.41,0.678191900253,2.0,aelmx, aryn3,cycliste_azelm,velo_azojn_1486042024.13,1486042052.51,1486042054.42,1.90497398376,1.41421356237,aelmx)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajets.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3884"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajets.distinct.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3) on peut faire apparaître des  tableaux "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(aefb2, cycliste_azex3, velo_azed9_1486042024.4, 1486042052.49, 1486042054.41, 1.92083001137, 1.41421356237, azyio)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val splitted = trajets.map(line => line.split(\",\"))\n",
    "splitted.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(Array(aefb2, cycliste_azex3, velo_azed9_1486042024.4, 1486042052.49, 1486042054.41, 1.92083001137, 1.41421356237, azyio))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(Array(aefb2, cycliste_azex3, velo_azed9_1486042024.4, 1486042052.49, 1486042054.41, 1.92083001137, 1.41421356237, azyio), Array(azux9, cycliste_azet7, velo_aepj0_1486042024.44, 1486042052.5, 1486042054.41, 1.91730213165, 3.16227766017, aefb2), Array(aelmx, cycliste_azem8, velo_aetin_1486042024.34, 1486042053.73, 1486042054.41, 0.683007001877, 1.0, azys7), Array(azrop, cycliste_azeu0, velo_ardb4_1486042024.19, 1486042053.74, 1486042054.41, 0.678191900253, 2.0, aelmx), Array(aryn3, cycliste_azelm, velo_azojn_1486042024.13, 1486042052.51, 1486042054.42, 1.90497398376, 1.41421356237, aelmx))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(Array(aefb2, cycliste_azex3, velo_azed9_1486042024.4, 1486042052.49, 1486042054.41, 1.92083001137, 1.41421356237, azyio), Array(azux9, cycliste_azet7, velo_aepj0_1486042024.44, 1486042052.5, 1486042054.41, 1.91730213165, 3.16227766017, aefb2), Array(aelmx, cycliste_azem8, velo_aetin_1486042024.34, 1486042053.73, 1486042054.41, 0.683007001877, 1.0, azys7), Array(azrop, cycliste_azeu0, velo_ardb4_1486042024.19, 1486042053.74, 1486042054.41, 0.678191900253, 2.0, aelmx), Array(aryn3, cycliste_azelm, velo_azojn_1486042024.13, 1486042052.51, 1486042054.42, 1.90497398376, 1.41421356237, aelmx), Array(azu15, cycliste_azel6, velo_arilw_1486042024.41, 1486042053.75, 1486042054.42, 0.669826984406, 1.0, aryuw), Array(azux9, cycliste_azepw, velo_aegm0_1486042024...."
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4) on peut récupèrer un tableau qui contient tous les élèments des différents tableaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[64] at flatMap at <console>:41"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trajets_like = trajets.flatMap(line => line.split(\",\")) // Rdd\n",
    "trajets_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(aefb2, cycliste_azex3, velo_azed9_1486042024.4, 1486042052.49)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajets_like.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31072"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajets_like.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7354"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajets_like.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5) on peut extraire un sous set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val pourcentage = 0.01\n",
    "val avec_remise = false\n",
    "val subset = trajets_like.sample(avec_remise, pourcentage) // en pourcentage de la base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "285"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(cycliste_azel6, 1486042052.84, aryn3, 1486042054.0, 1486042054.06, aefb2, 1486042055.36, 1.41421356237, aetlb, az359, 1486042085.6, velo_azos9_1486042084.01, cycliste_azegm, aedx3, 1486042089.73, azsg2, azghk, 0.617660999298, cycliste_azeq0, 1486042089.11, azsm0, 1486042089.77, velo_aerl6_1486042084.19, 1486042088.98, 1.00485610962, azsm0, cycliste_azeqv, azsg2, 1.41421356237, 3.16227766017, cycliste_azerh, 1486042091.12, aeisc, aetlb, velo_azhkc_1486042083.85, 0.549829006195, velo_azpgl_1486042084.0, aehkw, aetlb, arpvb, aetlb, velo_aeuj4_1486042084.09, velo_arsh5_1486042084.1, aryp0, 1486042091.4, aryb9, azh59, velo_azerk_1486042084.28, 2.2360679775, aedx3, 2.2360679775, ael47, 1486042092.15, 0.545649051666, 2.0, 0.450442075729, 1486042092.43, azghk, ael4..."
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(aryb9,cycliste_azeq9,velo_aeqsc_1486042084.24,1486042111.53,1486042111.98,0.452876091003,0.0,aryb9, ael47,cycliste_azew2,velo_aeiv1_1486042084.21,1486042095.82,1486042097.13,1.30658793449,3.16227766017,aetc2, azh59,cycliste_azert,velo_azeum_1486042084.13,1486042106.6,1486042108.18,1.57330083847,3.16227766017,aryb9, azghk,cycliste_azep1,velo_aryq1_1486042084.25,1486042108.72,1486042110.05,1.33539891243,3.60555127546,aedx3, aeyp3,cycliste_azeiq,velo_az360_1486042084.03,1486042107.46,1486042108.04,0.579392910004,1.0,ael47, aedk4,cycliste_azerw,velo_azdmb_1486042084.25,1486042093.28,1486042093.97,0.690509080887,1.41421356237,aetlb, azghk,cycliste_azeyx,velo_aejn5_1486042083.93,1486042102.3,1486042102.62,0.314414978027,2.2360679775,azsm0, aew40,cycliste_azeum,ve..."
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// en nombre de valeurs\n",
    "trajets.takeSample(false, 100) // action => DAG calculé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(aefb2,cycliste_azex3,velo_azed9_1486042024.4,1486042052.49,1486042054.41,1.92083001137,1.41421356237,azyio, azux9,cycliste_azet7,velo_aepj0_1486042024.44,1486042052.5,1486042054.41,1.91730213165,3.16227766017,aefb2, aelmx,cycliste_azem8,velo_aetin_1486042024.34,1486042053.73,1486042054.41,0.683007001877,1.0,azys7, azrop,cycliste_azeu0,velo_ardb4_1486042024.19,1486042053.74,1486042054.41,0.678191900253,2.0,aelmx)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajets.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.6 On peut faire des calculs de base sur des colones numériques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val durees  = trajets.map(l => l.split(\",\")).map(l => (l(5).toFloat))\n",
    "val kmtrage = trajets.map(l => l.split(\",\")).map(l => (l(6).toFloat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0030059814\n",
      "0.9055036855161382\n",
      "5.544328\n"
     ]
    }
   ],
   "source": [
    "System.out.println(durees.min)\n",
    "System.out.println(durees.mean)\n",
    "System.out.println(durees.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "2.1012103067735923\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "System.out.println(kmtrage.min)\n",
    "System.out.println(kmtrage.mean)\n",
    "System.out.println(kmtrage.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) seconde RDD : cyclistes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val path      = \"./logs/cycliste_cyclistes.csv\"\n",
    "// cyclistes est une RDD\n",
    "val cyclistes = sc.textFile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(cycliste_azepf,-0.5,43,femme,13.0136753551,13.9717095482,0.5)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cyclistes.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1) transformation en pairedRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// la colonne 0 donne le nom du cycliste, la n° 3 le genre\n",
    "val cycliste_genre = cyclistes.map( _.split(\",\")).map( r => (r(0), r(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((cycliste_azepf,femme))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cycliste_genre.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2) transformation des trajets en pairedRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// la colonne 1 donne le nom du cycliste, la 6 le nombre de kilometre\n",
    "val k_v_trajets = trajets.map(_.split(\",\")).map(ligne => ( ligne(1), ligne(6) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((cycliste_azex3,1.41421356237))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_v_trajets.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3) jointure sur la clef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val trajets_des_cyclistes = cycliste_genre.join(k_v_trajets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((cycliste_azeu8,(homme,3.60555127546)), (cycliste_azeu8,(homme,1.41421356237)), (cycliste_azeu8,(homme,3.60555127546)), (cycliste_azeu8,(homme,3.60555127546)), (cycliste_azeu8,(homme,3.60555127546)), (cycliste_azeu8,(homme,3.60555127546)), (cycliste_azeu8,(homme,1.41421356237)), (cycliste_azeu8,(homme,1.41421356237)), (cycliste_azeu8,(homme,1.0)), (cycliste_azeu8,(homme,2.82842712475)))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajets_des_cyclistes.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4) analyse par clef (ici le cycliste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((cycliste_azeu8,CompactBuffer((homme,3.60555127546), (homme,1.41421356237), (homme,3.60555127546), (homme,3.60555127546), (homme,3.60555127546), (homme,3.60555127546), (homme,1.41421356237), (homme,1.41421356237), (homme,1.0), (homme,2.82842712475), (homme,1.41421356237), (homme,5.0), (homme,1.0), (homme,2.82842712475))), (cycliste_azei4,CompactBuffer((femme,1.41421356237))))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trajets_par_cycliste = trajets_des_cyclistes.groupByKey()\n",
    "trajets_par_cycliste.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val a = trajets_par_cycliste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((cycliste_azeu8,36.34146487628001), (cycliste_azei4,1.41421356237), (cycliste_azej9,34.40093887518))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.map(x => (x._1 , x._2.map({case (k,v) => v.toDouble}).sum)).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((cycliste_azeu8,1.0), (cycliste_azei4,1.41421356237), (cycliste_azej9,1.0))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mini = a.map(x => (x._1 , x._2.map({case (k,v) => v.toDouble}).min))\n",
    "mini.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((cycliste_azeu8,5.0), (cycliste_azei4,1.41421356237), (cycliste_azej9,3.60555127546))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val maxi = a.map(x => (x._1 , x._2.map({case (k,v) => v.toDouble}).max))\n",
    "maxi.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((cycliste_azeu8,36.34146487628001), (cycliste_azei4,1.41421356237), (cycliste_azej9,34.40093887518))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nbKm = a.map(x => (x._1 , x._2.map({case (k,v) => v.toDouble}).sum))\n",
    "nbKm.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((cycliste_azeu8,14), (cycliste_azei4,1), (cycliste_azej9,12))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nbTrajet = a.map(x => (x._1 , x._2.map({case (k,v) => v.toDouble}).size))\n",
    "nbTrajet.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((cycliste_azeu8,(14,36.34146487628001)), (cycliste_azei4,(1,1.41421356237)), (cycliste_azej9,(12,34.40093887518)))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val jointure = nbTrajet.join(nbKm)\n",
    "jointure.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((cycliste_azeu8,2.595818919734286), (cycliste_azei4,1.41421356237), (cycliste_azej9,2.866744906265))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nb_km_moyen = jointure.map(data => (data._1, data._2._2 / data._2._1))\n",
    "nb_km_moyen.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val path = \"./logs_backup/toto.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val cyclistes = spark.read.option(\"header\", \"true\").csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) lire un csv renvoie une dataframe de row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+---+-----+-------------+---------------+-------+\n",
      "|           nom|niveau_sportif|age|genre|     km_moyen|vitesse_moyenne|attente|\n",
      "+--------------+--------------+---+-----+-------------+---------------+-------+\n",
      "|cycliste_azeqv|             2| 48|femme|10.2181585367|   17.634070191|    0.5|\n",
      "|cycliste_aztn9|             4| 17|femme|7.22914973386|  22.3342205623|    0.5|\n",
      "|cycliste_azrw3|             6| 30|homme|4.12219368148|  27.7083743417|    0.5|\n",
      "+--------------+--------------+---+-----+-------------+---------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cyclistes.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|genre|count|\n",
      "+-----+-----+\n",
      "|homme|  134|\n",
      "|femme|  166|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cyclistes.groupBy('genre).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|genre|count|\n",
      "+-----+-----+\n",
      "|femme|  166|\n",
      "|homme|  134|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cyclistes.groupBy('genre).count().orderBy('count desc).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+-----+\n",
      "|genre|niveau_sportif|count|\n",
      "+-----+--------------+-----+\n",
      "|femme|             4|   41|\n",
      "|femme|             6|   38|\n",
      "|homme|             2|   36|\n",
      "|femme|             2|   33|\n",
      "|femme|          -0.5|   32|\n",
      "|homme|             0|   27|\n",
      "|homme|             4|   27|\n",
      "|homme|             6|   23|\n",
      "|femme|             0|   22|\n",
      "|homme|          -0.5|   21|\n",
      "+-----+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cyclistes.groupBy('genre, 'niveau_sportif).count().orderBy('count desc, 'genre, 'niveau_sportif ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val hommes = cyclistes.filter('genre contains \"homme\")\n",
    "hommes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val voyageurs = hommes.filter('km_moyen > 10)\n",
    "voyageurs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+---+-------------+---------------+-------+\n",
      "|           nom|niveau_sportif|age|     km_moyen|vitesse_moyenne|attente|\n",
      "+--------------+--------------+---+-------------+---------------+-------+\n",
      "|cycliste_azeqv|             2| 48|10.2181585367|   17.634070191|    0.5|\n",
      "|cycliste_aztn9|             4| 17|7.22914973386|  22.3342205623|    0.5|\n",
      "|cycliste_azrw3|             6| 30|4.12219368148|  27.7083743417|    0.5|\n",
      "+--------------+--------------+---+-------------+---------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val assexue = cyclistes.drop('genre)\n",
    "assexue.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+---+-------------+---------------+-------+\n",
      "|           nom|niveau_sportif|age|     km_moyen|vitesse_moyenne|attente|\n",
      "+--------------+--------------+---+-------------+---------------+-------+\n",
      "|cycliste_azfc3|             4| 80|7.83212039083|  9.54151536677|    0.5|\n",
      "|cycliste_azts4|             0| 80|2.65763170426|  9.85875838863|    0.5|\n",
      "|cycliste_azdkx|             0| 80| 6.5845108176|  20.8670725418|    0.5|\n",
      "+--------------+--------------+---+-------------+---------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assexue.sort('age desc).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+---+-------------+---------------+-------+--------+\n",
      "|           nom|niveau_sportif|age|     km_moyen|vitesse_moyenne|attente|voyageur|\n",
      "+--------------+--------------+---+-------------+---------------+-------+--------+\n",
      "|cycliste_azeqv|             2| 48|10.2181585367|   17.634070191|    0.5|    true|\n",
      "|cycliste_azyuo|             4| 20|12.3520434918|  24.8977315731|    0.5|    true|\n",
      "|cycliste_azty7|             6| 67|13.1309960304|  19.4322595127|    0.5|    true|\n",
      "+--------------+--------------+---+-------------+---------------+-------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assexue.withColumn(\"voyageur\", 'km_moyen > 10).filter('voyageur).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enregistrer la DataFrame en tant que table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val table_name = \"cyclistes\"\n",
    "cyclistes.registerTempTable(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regarder les tables existante dans  le sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(cyclistes)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.tableNames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requêter une dataframe avec du SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val trajets = spark.read.option(\"header\", \"true\").csv(\"./logs/trajets.csv\") // Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trajets.registerTempTable(\"trajets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+-----------------------+-------------+-------------+--------------+-------------+-----+\n",
      "|aefb2|cycliste_azex3|velo_azed9_1486042024.4|1486042052.49|1486042054.41| 1.92083001137|1.41421356237|azyio|\n",
      "+-----+--------------+-----------------------+-------------+-------------+--------------+-------------+-----+\n",
      "|azux9|cycliste_azet7|   velo_aepj0_148604...| 1486042052.5|1486042054.41| 1.91730213165|3.16227766017|aefb2|\n",
      "|aelmx|cycliste_azem8|   velo_aetin_148604...|1486042053.73|1486042054.41|0.683007001877|          1.0|azys7|\n",
      "|azrop|cycliste_azeu0|   velo_ardb4_148604...|1486042053.74|1486042054.41|0.678191900253|          2.0|aelmx|\n",
      "+-----+--------------+-----------------------+-------------+-------------+--------------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val requete = s\"Select * from trajets limit 3\"\n",
    "sqlContext.sql(requete).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### requete 01 : compte le nb de lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    3883|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val requete = s\"Select count(*) from trajets\"\n",
    "sqlContext.sql(requete).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### requete 02 : nb de cyclistes distincts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.AnalysisException\n",
       "Message: cannot resolve '`cycliste`' given input columns: [aefb2, 1.41421356237, 1486042054.41, velo_azed9_1486042024.4, cycliste_azex3, azyio, 1.92083001137, 1486042052.49]; line 1 pos 23;\n",
       "'Project [unresolvedalias('count('cycliste), None)]\n",
       "+- SubqueryAlias trajets\n",
       "   +- Relation[aefb2#301,cycliste_azex3#302,velo_azed9_1486042024.4#303,1486042052.49#304,1486042054.41#305,1.92083001137#306,1.41421356237#307,azyio#308] csv\n",
       "\n",
       "StackTrace: 'Project [unresolvedalias('count('cycliste), None)]\n",
       "+- SubqueryAlias trajets\n",
       "   +- Relation[aefb2#301,cycliste_azex3#302,velo_azed9_1486042024.4#303,1486042052.49#304,1486042054.41#305,1.92083001137#306,1.41421356237#307,azyio#308] csv\n",
       "\n",
       "  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:77)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:74)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)\n",
       "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5$$anonfun$apply$11.apply(TreeNode.scala:357)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "  at scala.collection.AbstractTraversable.map(Traversable.scala:104)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:355)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:269)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:279)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2$1.apply(QueryPlan.scala:283)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.immutable.List.foreach(List.scala:381)\n",
       "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "  at scala.collection.immutable.List.map(List.scala:285)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:283)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$8.apply(QueryPlan.scala:288)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:288)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:74)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:67)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:67)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:58)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)\n",
       "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)\n",
       "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:582)\n",
       "  at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:682)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val requete = s\"select count( distinct cycliste ) from trajets\"\n",
    "sqlContext.sql(requete).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### requete 03 : distance moyenne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.AnalysisException\n",
       "Message: cannot resolve '`nb_km`' given input columns: [aefb2, 1.41421356237, 1486042054.41, velo_azed9_1486042024.4, cycliste_azex3, azyio, 1.92083001137, 1486042052.49]; line 1 pos 13;\n",
       "'Project ['mean('nb_km) AS nb_km_moyen#357]\n",
       "+- SubqueryAlias trajets\n",
       "   +- Relation[aefb2#301,cycliste_azex3#302,velo_azed9_1486042024.4#303,1486042052.49#304,1486042054.41#305,1.92083001137#306,1.41421356237#307,azyio#308] csv\n",
       "\n",
       "StackTrace: 'Project ['mean('nb_km) AS nb_km_moyen#357]\n",
       "+- SubqueryAlias trajets\n",
       "   +- Relation[aefb2#301,cycliste_azex3#302,velo_azed9_1486042024.4#303,1486042052.49#304,1486042054.41#305,1.92083001137#306,1.41421356237#307,azyio#308] csv\n",
       "\n",
       "  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:77)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:74)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)\n",
       "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5$$anonfun$apply$11.apply(TreeNode.scala:357)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "  at scala.collection.AbstractTraversable.map(Traversable.scala:104)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:355)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:269)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:279)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2$1.apply(QueryPlan.scala:283)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.immutable.List.foreach(List.scala:381)\n",
       "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "  at scala.collection.immutable.List.map(List.scala:285)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:283)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$8.apply(QueryPlan.scala:288)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:288)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:74)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:67)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:67)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:58)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)\n",
       "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)\n",
       "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:582)\n",
       "  at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:682)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// performance moyenne : \n",
    "val requete = s\"\"\"Select mean( nb_km) as nb_km_moyen\n",
    "                  from  trajets\"\"\"\n",
    "sqlContext.sql(requete).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### requete 05 : conversion de timestamp en date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.AnalysisException\n",
       "Message: cannot resolve '`cycliste`' given input columns: [aefb2, 1.41421356237, 1486042054.41, velo_azed9_1486042024.4, cycliste_azex3, azyio, 1.92083001137, 1486042052.49]; line 7 pos 17;\n",
       "'Aggregate ['cycliste], ['cycliste, 'min('from_unixtime('heure_de_depart, YYYY-MM-dd HH:mm:ss)) AS premiere_sortie#358, 'max('from_unixtime('heure_d_arrivee, YYYY-MM-dd HH:mm:ss)) AS derniere_sortie#359, 'count('heure_de_depart) AS nb_sortie#360]\n",
       "+- SubqueryAlias trajets\n",
       "   +- Relation[aefb2#301,cycliste_azex3#302,velo_azed9_1486042024.4#303,1486042052.49#304,1486042054.41#305,1.92083001137#306,1.41421356237#307,azyio#308] csv\n",
       "\n",
       "StackTrace: 'Aggregate ['cycliste], ['cycliste, 'min('from_unixtime('heure_de_depart, YYYY-MM-dd HH:mm:ss)) AS premiere_sortie#358, 'max('from_unixtime('heure_d_arrivee, YYYY-MM-dd HH:mm:ss)) AS derniere_sortie#359, 'count('heure_de_depart) AS nb_sortie#360]\n",
       "+- SubqueryAlias trajets\n",
       "   +- Relation[aefb2#301,cycliste_azex3#302,velo_azed9_1486042024.4#303,1486042052.49#304,1486042054.41#305,1.92083001137#306,1.41421356237#307,azyio#308] csv\n",
       "\n",
       "  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:77)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:74)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)\n",
       "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:269)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:279)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2$1.apply(QueryPlan.scala:283)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "  at scala.collection.AbstractTraversable.map(Traversable.scala:104)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:283)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$8.apply(QueryPlan.scala:288)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:288)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:74)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:67)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:67)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:58)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)\n",
       "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)\n",
       "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:582)\n",
       "  at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:682)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val requete = s\"\"\"\n",
    "Select  cycliste,\n",
    "        min(from_unixtime(heure_de_depart,'YYYY-MM-dd HH:mm:ss')) as premiere_sortie, \n",
    "        max(from_unixtime(heure_d_arrivee,'YYYY-MM-dd HH:mm:ss')) as derniere_sortie,\n",
    "        count(distinct heure_de_depart) as nb_sortie\n",
    "        from trajets\n",
    "        group by cycliste\n",
    "\"\"\"\n",
    "sqlContext.sql(requete).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### requete 06 : imbrication de requêtes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.AnalysisException\n",
       "Message: cannot resolve '`cycliste`' given input columns: [aefb2, 1.41421356237, 1486042054.41, velo_azed9_1486042024.4, cycliste_azex3, azyio, 1.92083001137, 1486042052.49]; line 9 pos 17;\n",
       "'GlobalLimit 3\n",
       "+- 'LocalLimit 3\n",
       "   +- 'Project ['cycliste, 'from_unixtime(('derniere_sortie - 'premiere_sortie), HH:mm:ss) AS anciennete#363]\n",
       "      +- 'Aggregate ['cycliste], ['cycliste, 'min('heure_de_depart) AS premiere_sortie#361, 'max('heure_d_arrivee) AS derniere_sortie#362]\n",
       "         +- SubqueryAlias trajets\n",
       "            +- Relation[aefb2#301,cycliste_azex3#302,velo_azed9_1486042024.4#303,1486042052.49#304,1486042054.41#305,1.92083001137#306,1.41421356237#307,azyio#308] csv\n",
       "\n",
       "StackTrace: 'GlobalLimit 3\n",
       "+- 'LocalLimit 3\n",
       "   +- 'Project ['cycliste, 'from_unixtime(('derniere_sortie - 'premiere_sortie), HH:mm:ss) AS anciennete#363]\n",
       "      +- 'Aggregate ['cycliste], ['cycliste, 'min('heure_de_depart) AS premiere_sortie#361, 'max('heure_d_arrivee) AS derniere_sortie#362]\n",
       "         +- SubqueryAlias trajets\n",
       "            +- Relation[aefb2#301,cycliste_azex3#302,velo_azed9_1486042024.4#303,1486042052.49#304,1486042054.41#305,1.92083001137#306,1.41421356237#307,azyio#308] csv\n",
       "\n",
       "  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:77)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:74)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)\n",
       "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:269)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:279)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2$1.apply(QueryPlan.scala:283)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "  at scala.collection.AbstractTraversable.map(Traversable.scala:104)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:283)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$8.apply(QueryPlan.scala:288)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:288)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:74)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:67)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n",
       "  at scala.collection.immutable.List.foreach(List.scala:381)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n",
       "  at scala.collection.immutable.List.foreach(List.scala:381)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n",
       "  at scala.collection.immutable.List.foreach(List.scala:381)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:67)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:58)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)\n",
       "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)\n",
       "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:582)\n",
       "  at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:682)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val requete = s\"\"\"\n",
    "Select  cycliste, \n",
    "        from_unixtime(derniere_sortie - premiere_sortie, 'HH:mm:ss') as anciennete\n",
    "from\n",
    "(       Select  cycliste,\n",
    "                min(heure_de_depart) as premiere_sortie, \n",
    "                max(heure_d_arrivee) as derniere_sortie                \n",
    "        from trajets\n",
    "        group by cycliste\n",
    "        )\n",
    "        limit 3\n",
    "\"\"\"\n",
    "sqlContext.sql(requete).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3) jointure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:36: error: not found: value StructField\n",
       "       var champs         = List(  StructField(\"nom\"    , StringType, true) ,\n",
       "                                   ^\n",
       "<console>:36: error: not found: value StringType\n",
       "       var champs         = List(  StructField(\"nom\"    , StringType, true) ,\n",
       "                                                          ^\n",
       "<console>:37: error: not found: value StructField\n",
       "                                   StructField(\"heure\"  , StringType, true) ,\n",
       "                                   ^\n",
       "<console>:37: error: not found: value StringType\n",
       "                                   StructField(\"heure\"  , StringType, true) ,\n",
       "                                                          ^\n",
       "<console>:38: error: not found: value StructField\n",
       "                                   StructField(\"velo\"   , StringType, true) ,\n",
       "                                   ^\n",
       "<console>:38: error: not found: value StringType\n",
       "                                   StructField(\"velo\"   , StringType, true) ,\n",
       "                                                          ^\n",
       "<console>:39: error: not found: value StructField\n",
       "                                   StructField(\"action\" , StringType, true) )\n",
       "                                   ^\n",
       "<console>:39: error: not found: value StringType\n",
       "                                   StructField(\"action\" , StringType, true) )\n",
       "                                                          ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// 1) création de la structure\n",
    "var champs         = List(  StructField(\"nom\"    , StringType, true) ,\n",
    "                            StructField(\"heure\"  , StringType, true) ,\n",
    "                            StructField(\"velo\"   , StringType, true) ,\n",
    "                            StructField(\"action\" , StringType, true) )\n",
    "var schema         = StructType(champs)\n",
    "\n",
    "// 2) lecture des données\n",
    "val path           = \"./logs_backup/cycliste_prises.csv\"\n",
    "val DataSet_prise  = sqlContext.read.schema(schema).csv(path)\n",
    "\n",
    "// 3) enregistrement de la table\n",
    "DataSet_prise.registerTempTable(\"prises\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:35: error: not found: value StructField\n",
       "       var champs = List(  StructField(\"cyclise\"  , StringType, true),\n",
       "                           ^\n",
       "<console>:35: error: not found: value StringType\n",
       "       var champs = List(  StructField(\"cyclise\"  , StringType, true),\n",
       "                                                    ^\n",
       "<console>:36: error: not found: value StructField\n",
       "                           StructField(\"heure\"    , StringType, true),\n",
       "                           ^\n",
       "<console>:36: error: not found: value StringType\n",
       "                           StructField(\"heure\"    , StringType, true),\n",
       "                                                    ^\n",
       "<console>:37: error: not found: value StructField\n",
       "                           StructField(\"rendu\"    , StringType, true),\n",
       "                           ^\n",
       "<console>:37: error: not found: value StringType\n",
       "                           StructField(\"rendu\"    , StringType, true),\n",
       "                                                    ^\n",
       "<console>:38: error: not found: value StructField\n",
       "                           StructField(\"duree\"    , StringType, true),\n",
       "                           ^\n",
       "<console>:38: error: not found: value StringType\n",
       "                           StructField(\"duree\"    , StringType, true),\n",
       "                                                    ^\n",
       "<console>:39: error: not found: value StructField\n",
       "                           StructField(\"velo\"     , StringType, true))\n",
       "                           ^\n",
       "<console>:39: error: not found: value StringType\n",
       "                           StructField(\"velo\"     , StringType, true))\n",
       "                                                    ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var champs = List(  StructField(\"cyclise\"  , StringType, true),\n",
    "                    StructField(\"heure\"    , StringType, true),\n",
    "                    StructField(\"rendu\"    , StringType, true),\n",
    "                    StructField(\"duree\"    , StringType, true),\n",
    "                    StructField(\"velo\"     , StringType, true))\n",
    "var r_schema      = StructType(champs)\n",
    "val DataSet_rendu = sqlContext.read.schema(r_schema).csv(\"./logs_backup/cycliste_rendu.csv\")\n",
    "DataSet_rendu.registerTempTable(\"rendus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.AnalysisException\n",
       "Message: Table or view not found: prises; line 5 pos 32\n",
       "StackTrace:   at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:456)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:475)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:460)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)\n",
       "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:60)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:58)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:58)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:58)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:460)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:450)\n",
       "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)\n",
       "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)\n",
       "  at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n",
       "  at scala.collection.immutable.List.foldLeft(List.scala:84)\n",
       "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)\n",
       "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)\n",
       "  at scala.collection.immutable.List.foreach(List.scala:381)\n",
       "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:65)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:63)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:51)\n",
       "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)\n",
       "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:582)\n",
       "  at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:682)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val requete = s\"\"\"  SELECT distinct  p.nom     , \n",
    "                                     p.heure   , r.heure   ,\n",
    "                                     r.rendu   , r.duree   ,\n",
    "                                     p.heure   , r.velo\n",
    "                    FROM        prises AS p \n",
    "                    INNER JOIN  rendus AS r \n",
    "                    ON          p.velo = r.velo\n",
    "\"\"\"\n",
    "sqlContext.sql(requete).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
