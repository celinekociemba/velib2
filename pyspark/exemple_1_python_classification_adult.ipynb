{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -1) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Python\n",
    "## modules\n",
    "import time  \n",
    "import sys  \n",
    "import numpy as np \n",
    "import copy\n",
    "## fonctions\n",
    "from datetime                  import datetime\n",
    "from dateutil.relativedelta    import relativedelta\n",
    "\n",
    "# Spark 1.6\n",
    "from pyspark                   import SparkContext \n",
    "from pyspark                   import SparkConf    \n",
    "from pyspark.sql               import SQLContext   \n",
    "from pyspark.sql               import HiveContext  \n",
    "\n",
    "# Spark 2.0\n",
    "from pyspark.sql               import SparkSession \n",
    "\n",
    "# Fonctions\n",
    "from pyspark.sql               import Row\n",
    "from pyspark.sql.types         import *\n",
    "\n",
    "# Machine learning\n",
    "from pyspark.ml                import Pipeline\n",
    "from pyspark.ml.feature        import OneHotEncoder\n",
    "from pyspark.ml.feature        import StringIndexer\n",
    "from pyspark.ml.feature        import VectorIndexer\n",
    "from pyspark.ml.feature        import VectorAssembler\n",
    "from pyspark.ml.evaluation     import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Ancienne librairie de ML\n",
    "from pyspark.mllib.evaluation  import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "app_name    = \"Random forest RF\"\n",
    "nb_cores    = 3\n",
    "paralelisme = 3\n",
    "memory      = 3\n",
    "start_load  = time.time()\n",
    "spark_1_6   = False\n",
    "spark_2     = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert( spark_1_6 & spark_2 == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#spark 1.6\n",
    "if spark_1_6:\n",
    "    conf = SparkConf()\n",
    "    conf.setAppName(app_name)\n",
    "    conf.set(\"spark.mesos.coarse\"             , \"True\")\n",
    "    conf.set(\"spark.executor.memory\"          , \"%sg\"%memory)\n",
    "    conf.set(\"spark.driver.memory\"            , \"%sg\"%memory)\n",
    "    conf.set(\"spark.serializer\"               , \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    conf.set(\"spark.kryoserializer.buffer.max\", \"1024m\")\n",
    "    conf.set(\"spark.driver.maxResultSize\"     , \"10g\")\n",
    "    conf.set(\"spark.cores.max\"                , \"%s\"%(nb_cores))\n",
    "    conf.set(\"spark.default.parallelism\"      , \"%s\"%(nb_cores*paralelisme))\n",
    "    conf.set(\"spark.storage.memoryFraction\"   , \"0.5\")\n",
    "    sc         = SparkContext(conf=conf)\n",
    "    sqlContext = HiveContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if spark_2:\n",
    "    spark = SparkSession.builder\\\n",
    "    .config(\"spark.app.name\"                  , app_name                                   )\\\n",
    "    .config(\"spark.cores.max\"                 , \"%s\"%(nb_cores)                            )\\\n",
    "    .config(\"spark.mesos.coarse\"             , \"True\"                                      )\\\n",
    "    .config(\"spark.executor.memory\"          , \"%sg\"%memory                                )\\\n",
    "    .config(\"spark.driver.memory\"            , \"%sg\"%memory                                )\\\n",
    "    .config(\"spark.serializer\"               , \"org.apache.spark.serializer.KryoSerializer\")\\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\"                                     )\\\n",
    "    .config(\"spark.driver.maxResultSize\"     , \"10g\"                                       )\\\n",
    "    .config(\"spark.cores.max\"                , \"%s\"%(nb_cores)                             )\\\n",
    "    .config(\"spark.default.parallelism\"      , \"%s\"%(nb_cores*paralelisme)                 )\\\n",
    "    .config(\"spark.storage.memoryFraction\"   , \"0.5\"                                       )\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Structure et import du fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colY = \"income\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adultschema = StructType([\n",
    "    StructField(\"age\"            , DoubleType() ,True),\n",
    "    StructField(\"occupation\"     , StringType() ,True),\n",
    "    StructField(\"capital_gain\"   , DoubleType() ,True),\n",
    "    StructField(\"education\"      , StringType() ,True),\n",
    "    StructField(\"marital_status\" , StringType() ,True),    \n",
    "    StructField(\"workclass\"      , StringType() ,True),\n",
    "    StructField(\"relationship\"   , StringType() ,True),\n",
    "    StructField(\"race\"           , StringType() ,True),\n",
    "    StructField(\"sex\"            , StringType() ,True),\n",
    "    StructField(\"capital_loss\"   , DoubleType() ,True),\n",
    "    StructField(\"fnlwgt\"         , DoubleType() ,True),\n",
    "    StructField(\"hours_per_week\" , DoubleType() ,True),\n",
    "    StructField(\"native_country\" , StringType() ,True),\n",
    "    StructField(\"income\"         , StringType() ,True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"/Users/romain/Desktop/adult.raw.txt\", schema=adultschema, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(age=39.0, occupation=u' State-gov', capital_gain=77516.0, education=u' Bachelors', marital_status=u' Never-married', workclass=u' Adm-clerical', relationship=u' Not-in-family', race=u' White', sex=u' Male', capital_loss=2174.0, fnlwgt=0.0, hours_per_week=40.0, native_country=u' United-States', income=u' <=50K'), Row(age=50.0, occupation=u' Self-emp-not-inc', capital_gain=83311.0, education=u' Bachelors', marital_status=u' Married-civ-spouse', workclass=u' Exec-managerial', relationship=u' Husband', race=u' White', sex=u' Male', capital_loss=0.0, fnlwgt=0.0, hours_per_week=13.0, native_country=u' United-States', income=u' <=50K')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48842"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print data.take(2)\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#si on veut vérifier les choses\n",
    "if False:\n",
    "    for col in data.columns:\n",
    "        data.groupby(col).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) gestion des  colonnes catégorielles / numériques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indexStringColumns(df, cols):\n",
    "    \"\"\"\n",
    "    Convertit les colonnes de string en numériques.\n",
    "    (pbm = donne l'idée d'un ordre naturel)\n",
    "    Parameters:\n",
    "        df : matrice à modifier \n",
    "            dataframe\n",
    "        cols : noms des colonnes à indexer\n",
    "            list de chaine de caractère\n",
    "            \n",
    "    Return: dataframe\n",
    "    \"\"\"\n",
    "    from pyspark.ml.feature import StringIndexer\n",
    "    newdf = df\n",
    "    for col in cols:\n",
    "        indexer = StringIndexer(inputCol=col, outputCol=col+\"-num\")\n",
    "        model   = indexer.fit(newdf)\n",
    "        newdf   = model.transform(newdf).drop(col)\n",
    "        newdf   = newdf.withColumnRenamed(col+\"-num\", col)\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oneHotEncodeColumns(df, cols):\n",
    "    \"\"\"\n",
    "    Convertit une colonne contenant n modalité \n",
    "    en n colonne ne comprenant qu'une seule valeur.\n",
    "    (Supprime les effets d'ordre des valeurs numériques)\n",
    "    Parameters:\n",
    "        df : matrice à modifier \n",
    "            dataframe\n",
    "        cols : noms des colonnes à indexer\n",
    "            list de chaine de caractère\n",
    "            \n",
    "    Return: dataframe\n",
    "    \"\"\"\n",
    "    from pyspark.ml.feature import OneHotEncoder\n",
    "    newdf = df\n",
    "    for col in cols:\n",
    "        onehotenc = OneHotEncoder(inputCol=col, outputCol=col+\"-onehot\", dropLast=False)\n",
    "        newdf     = onehotenc.transform(newdf).drop(col)\n",
    "        newdf     = newdf.withColumnRenamed(col+\"-onehot\", col)\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1) catégories => numériques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'income'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "typeString = [x[0] for x in data.dtypes if x[1]=='string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['occupation',\n",
       " 'education',\n",
       " 'marital_status',\n",
       " 'workclass',\n",
       " 'relationship',\n",
       " 'race',\n",
       " 'sex',\n",
       " 'native_country',\n",
       " 'income']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typeString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colString_without_Y = copy.copy(typeString)\n",
    "colString_without_Y.remove(colY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=39.0, occupation=u' State-gov', capital_gain=77516.0, education=u' Bachelors', marital_status=u' Never-married', workclass=u' Adm-clerical', relationship=u' Not-in-family', race=u' White', sex=u' Male', capital_loss=2174.0, fnlwgt=0.0, hours_per_week=40.0, native_country=u' United-States', income=u' <=50K')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=39.0, capital_gain=77516.0, capital_loss=2174.0, fnlwgt=0.0, hours_per_week=40.0, occupation=4.0, education=2.0, marital_status=1.0, workclass=3.0, relationship=1.0, race=0.0, sex=0.0, native_country=0.0, income=0.0)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = indexStringColumns(data, typeString)\n",
    "data2.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2) numériques => one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "typeDouble = [x[0] for x in data2.dtypes if x[1]=='double']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=39.0, capital_gain=77516.0, capital_loss=2174.0, fnlwgt=0.0, hours_per_week=40.0, income=0.0, occupation=SparseVector(9, {4: 1.0}), education=SparseVector(16, {2: 1.0}), marital_status=SparseVector(7, {1: 1.0}), workclass=SparseVector(15, {3: 1.0}), relationship=SparseVector(6, {1: 1.0}), race=SparseVector(5, {0: 1.0}), sex=SparseVector(2, {0: 1.0}), native_country=SparseVector(42, {0: 1.0}))]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3 = oneHotEncodeColumns(data2, colString_without_Y)\n",
    "data3.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3) plusieurs colonnes => un vecteur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=39.0, capital_gain=77516.0, capital_loss=2174.0, fnlwgt=0.0, hours_per_week=40.0, income=0.0, occupation=SparseVector(9, {4: 1.0}), education=SparseVector(16, {2: 1.0}), marital_status=SparseVector(7, {1: 1.0}), workclass=SparseVector(15, {3: 1.0}), relationship=SparseVector(6, {1: 1.0}), race=SparseVector(5, {0: 1.0}), sex=SparseVector(2, {0: 1.0}), native_country=SparseVector(42, {0: 1.0}), features=SparseVector(107, {0: 39.0, 1: 77516.0, 2: 2174.0, 4: 40.0, 9: 1.0, 16: 1.0, 31: 1.0, 40: 1.0, 53: 1.0, 58: 1.0, 63: 1.0, 65: 1.0})),\n",
       " Row(age=50.0, capital_gain=83311.0, capital_loss=0.0, fnlwgt=0.0, hours_per_week=13.0, income=0.0, occupation=SparseVector(9, {1: 1.0}), education=SparseVector(16, {2: 1.0}), marital_status=SparseVector(7, {0: 1.0}), workclass=SparseVector(15, {2: 1.0}), relationship=SparseVector(6, {0: 1.0}), race=SparseVector(5, {0: 1.0}), sex=SparseVector(2, {0: 1.0}), native_country=SparseVector(42, {0: 1.0}), features=SparseVector(107, {0: 50.0, 1: 83311.0, 4: 13.0, 6: 1.0, 16: 1.0, 30: 1.0, 39: 1.0, 52: 1.0, 58: 1.0, 63: 1.0, 65: 1.0}))]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choix des colonnes\n",
    "features = data3.columns\n",
    "features.remove(colY)\n",
    "# Assembleur\n",
    "assemblor = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "# Application\n",
    "data4 = assemblor.transform(data3)\n",
    "data4.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    label = [colY]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4) projection, et cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(features,VectorUDT,true),StructField(income,DoubleType,true)))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data5 = data4.select(\"features\", colY)\n",
    "data5.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=SparseVector(107, {0: 39.0, 1: 77516.0, 2: 2174.0, 4: 40.0, 9: 1.0, 16: 1.0, 31: 1.0, 40: 1.0, 53: 1.0, 58: 1.0, 63: 1.0, 65: 1.0}), income=0.0),\n",
       " Row(features=SparseVector(107, {0: 50.0, 1: 83311.0, 4: 13.0, 6: 1.0, 16: 1.0, 30: 1.0, 39: 1.0, 52: 1.0, 58: 1.0, 63: 1.0, 65: 1.0}), income=0.0)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data5.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48842"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data5.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48790"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data6 = data5.dropDuplicates()\n",
    "data6.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5) équilibrage des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|income|count|\n",
      "+------+-----+\n",
      "|   0.0|37109|\n",
      "|   1.0|11681|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data6.groupBy(colY).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_examples = 10000\n",
    "income_0             = data6.filter(\"income == 0\")\n",
    "_10000_income_0      = income_0.sample(False, nb_examples/float(income_0.count()))\n",
    "\n",
    "income_1             = data6.filter(\"income == 1\")\n",
    "_10000_income_1      = income_1.sample(False, nb_examples/float(income_1.count()))\n",
    "\n",
    "classes_equilibrees  = _10000_income_0.union(_10000_income_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9990 10033 20023\n"
     ]
    }
   ],
   "source": [
    "print _10000_income_0.count(), _10000_income_1.count(), classes_equilibrees.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Division en jeu de test, et jeu d'apprentissage\n",
    "(trainingData, testData) = classes_equilibrees.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1) variation du nombre d'arbres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 arbre => Accuracy = 0.781745, Error = 0.218254630402\n",
      "2 arbre => Accuracy = 0.756382, Error = 0.243617553813\n",
      "3 arbre => Accuracy = 0.786417, Error = 0.213582512932\n",
      "4 arbre => Accuracy = 0.787419, Error = 0.212581344902\n",
      "5 arbre => Accuracy = 0.790589, Error = 0.209410979476\n",
      "6 arbre => Accuracy = 0.78575, Error = 0.214249958285\n",
      "7 arbre => Accuracy = 0.78041, Error = 0.219589521108\n",
      "8 arbre => Accuracy = 0.784415, Error = 0.21558484899\n",
      "9 arbre => Accuracy = 0.789087, Error = 0.21091273152\n",
      "10 arbre => Accuracy = 0.790589, Error = 0.209410979476\n",
      "11 arbre => Accuracy = 0.788754, Error = 0.211246454197\n",
      "12 arbre => Accuracy = 0.786251, Error = 0.21374937427\n",
      "13 arbre => Accuracy = 0.787085, Error = 0.212915067579\n",
      "14 arbre => Accuracy = 0.78308, Error = 0.216919739696\n",
      "15 arbre => Accuracy = 0.784582, Error = 0.215417987652\n",
      "16 arbre => Accuracy = 0.785917, Error = 0.214083096946\n",
      "17 arbre => Accuracy = 0.780244, Error = 0.219756382446\n",
      "18 arbre => Accuracy = 0.780744, Error = 0.219255798432\n",
      "19 arbre => Accuracy = 0.789421, Error = 0.210579008844\n"
     ]
    }
   ],
   "source": [
    "nb_max_arbre = 20\n",
    "for ntree in range(1,nb_max_arbre) :\n",
    "    rf          = RandomForestClassifier(labelCol=colY, numTrees=ntree)\n",
    "    model       = rf.fit(trainingData)\n",
    "    predictions = model.transform(testData)\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(  labelCol      = \"income\" , \n",
    "                                                    predictionCol = \"prediction\"   , \n",
    "                                                    metricName    = \"accuracy\"     )\n",
    "    accuracy  = evaluator.evaluate(predictions)\n",
    "    error     = 1 - accuracy\n",
    "\n",
    "    print(\"%s arbre => Accuracy = %g, Error = %s\" % (ntree, accuracy, error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.isLargerBetter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2) variation de la profondeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 arbre, depth = 5 => Error = 0.218255\n",
      "1 arbre, depth = 10 => Error = 0.200234\n",
      "1 arbre, depth = 20 => Error = 0.218421\n",
      "10 arbre, depth = 5 => Error = 0.209411\n",
      "10 arbre, depth = 10 => Error = 0.191557\n",
      "10 arbre, depth = 20 => Error = 0.175872\n",
      "20 arbre, depth = 5 => Error = 0.219923\n",
      "20 arbre, depth = 10 => Error = 0.194894\n",
      "20 arbre, depth = 20 => Error = 0.178542\n"
     ]
    }
   ],
   "source": [
    "test_forets = [1, 10, 20]\n",
    "test_depth  = [5, 10, 20]\n",
    "for ntree in test_forets:\n",
    "    for depth in test_depth:\n",
    "        rf          = RandomForestClassifier(labelCol=colY, numTrees=ntree, maxDepth=depth)\n",
    "        model       = rf.fit(trainingData)\n",
    "        predictions = model.transform(testData)\n",
    "\n",
    "        evaluator = MulticlassClassificationEvaluator(  labelCol      = \"income\" , \n",
    "                                                        predictionCol = \"prediction\"   , \n",
    "                                                        metricName    = \"accuracy\"     )\n",
    "        accuracy  = 1 - evaluator.evaluate(predictions)\n",
    "\n",
    "        print(\"%s arbre, depth = %s => Error = %g\" % (ntree, depth, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time as now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3) Etendue de la forêt\n",
    "(on peut aller prendre un café)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 arbre, depth = 30 => Error = 0.1765, duree = 3.44e+02 sec \n",
      "30 arbre, depth = 30 => Error = 0.1727, duree = 5.49e+02 sec \n",
      "50 arbre, depth = 30 => Error = 0.172, duree = 1.23e+03 sec \n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o4493.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 3238.0 failed 4 times, most recent failure: Lost task 6.3 in stage 3238.0 (TID 290118, 169.254.61.163): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1911)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:893)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:892)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:745)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:744)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:744)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.tpByClass$lzycompute(MulticlassMetrics.scala:48)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.tpByClass(MulticlassMetrics.scala:44)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.accuracy$lzycompute(MulticlassMetrics.scala:168)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.accuracy(MulticlassMetrics.scala:168)\n\tat org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator.evaluate(MulticlassClassificationEvaluator.scala:87)\n\tat sun.reflect.GeneratedMethodAccessor134.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-943b368624d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                                                           \u001b[0mpredictionCol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"prediction\"\u001b[0m   \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                                           metricName    = \"accuracy\"     )\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0maccuracy\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mduree\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdebut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} arbre, depth = {1} => Error = {2:3.4}, duree = {3:4.3} sec \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mntree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/romain/Informatique/zoo/spark-2.0.0-bin-hadoop2.6/python/pyspark/ml/evaluation.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/romain/Informatique/zoo/spark-2.0.0-bin-hadoop2.6/python/pyspark/ml/evaluation.pyc\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \"\"\"\n\u001b[1;32m     97\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0misLargerBetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/romain/Informatique/zoo/spark-2.0.0-bin-hadoop2.6/python/lib/py4j-0.10.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 933\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/romain/Informatique/zoo/spark-2.0.0-bin-hadoop2.6/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/romain/Informatique/zoo/spark-2.0.0-bin-hadoop2.6/python/lib/py4j-0.10.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    311\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o4493.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 3238.0 failed 4 times, most recent failure: Lost task 6.3 in stage 3238.0 (TID 290118, 169.254.61.163): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1911)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:893)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:892)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:745)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:744)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:744)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.tpByClass$lzycompute(MulticlassMetrics.scala:48)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.tpByClass(MulticlassMetrics.scala:44)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.accuracy$lzycompute(MulticlassMetrics.scala:168)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.accuracy(MulticlassMetrics.scala:168)\n\tat org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator.evaluate(MulticlassClassificationEvaluator.scala:87)\n\tat sun.reflect.GeneratedMethodAccessor134.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "test_forets = [ 20, 30,  50]\n",
    "test_depth  = [ 30] # limite à 30 de profondeurs \n",
    "for ntree in test_forets:\n",
    "    for depth in test_depth:\n",
    "        debut       = now()\n",
    "        # modélisation :\n",
    "        rf          = RandomForestClassifier(labelCol=colY, numTrees=ntree, maxDepth=depth,)\n",
    "        model       = rf.fit(trainingData)\n",
    "        predictions = model.transform(testData)\n",
    "        # mesure de la performance :\n",
    "        evaluator   = MulticlassClassificationEvaluator(  labelCol      = \"income\" , \n",
    "                                                          predictionCol = \"prediction\"   , \n",
    "                                                          metricName    = \"accuracy\"     )\n",
    "        accuracy     = 1 - evaluator.evaluate(predictions)\n",
    "        duree        = now()-debut\n",
    "        print(\"{0} arbre, depth = {1} => Error = {2:3.4}, duree = {3:5} sec \".format (ntree, depth, accuracy, duree))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tentative de récupération des features importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "    newdf       = copy.copy(data)\n",
    "    inputCol_1  = newdf.columns[1]\n",
    "    outputCol_1 = col+\"-num\"\n",
    "    indexer     = StringIndexer(inputCol=inputCol_1, outputCol=outputCol_1)\n",
    "    model       = indexer.fit(newdf)\n",
    "    newdf_1     = model.transform(newdf)\n",
    "    newdf_1.select(inputCol_1, outputCol_1).dropDuplicates().show()\n",
    "\n",
    "    inputCol_2   = outputCol_1\n",
    "    outputCol_2  = col+\"-onehot\"\n",
    "    onehotenc    = OneHotEncoder(inputCol=inputCol_2, outputCol=outputCol_2, dropLast=False)\n",
    "    newdf_2      = onehotenc.transform(newdf_1)\n",
    "    newdf_2.select(inputCol_1, outputCol_1, outputCol_2).dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) changement des classifieurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBTClassifier        => Accuracy = 0.825, Error = 0.175, duree = 1.09e+02 sec\n",
      "LogisticRegression   => Accuracy = 0.819, Error = 0.181, duree =  25.5 sec\n",
      "NaiveBayes           => Accuracy = 0.59, Error = 0.41, duree =  2.72 sec\n",
      "best_classifier = GBTClassifier, best_accuracy = 0.825129317537\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "classifiers = { \"NaiveBayes\"         : NaiveBayes(labelCol=colY)         , \n",
    "                \"GBTClassifier\"      : GBTClassifier(labelCol=colY)      , \n",
    "                \"LogisticRegression\" : LogisticRegression(labelCol=colY) }\n",
    "best_accuracy   = 0\n",
    "best_classifier = \"\"\n",
    "for classifierName,classifier in classifiers.iteritems():\n",
    "    debut       = now()\n",
    "    model       = classifier.fit(trainingData)\n",
    "    predictions = model.transform(testData)\n",
    "\n",
    "    evaluator   = MulticlassClassificationEvaluator(  labelCol      = \"income\" , \n",
    "                                                      predictionCol = \"prediction\"   , \n",
    "                                                      metricName    = \"accuracy\"     )\n",
    "    accuracy    = evaluator.evaluate(predictions)\n",
    "    error       = 1 - accuracy\n",
    "    duree       = now() - debut\n",
    "    print(\"{0:20} => Accuracy = {1:4.3}, Error = {2:4.3}, duree = {3:5.3} sec\".format (classifierName ,  \n",
    "                                                                                       accuracy       , \n",
    "                                                                                       error          , \n",
    "                                                                                       duree         ))\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy   = accuracy\n",
    "        best_classifier = classifierName\n",
    "        \n",
    "print \"best_classifier = %s, best_accuracy = %s\"%(best_classifier, best_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
